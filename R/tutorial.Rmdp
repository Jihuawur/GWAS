---
title: "Supplementary web-based tutorial to 'A guide to genome-wide association analysis and post-analytic interrogation, Statistics in Medicine, in review.'"
author: "Eric Reed, et al"
date: "May 12, 2015"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, warning=FALSE)
```

This tutorial presents fundamental concepts and specific software tools for implementing a complete genome wide association (GWA) analysis, as well as post-analytic visualization and interrogation of potentially novel findings.  In this tutorial we use complete GWA data on 1401 individuals from the [PennCATH study of coronary artery disease (CAD)](http://link-here/).

The tutorial follows the steps described in the following workflow diagram.

![GWA analysis workflow](figures/GWAS_workflow.png)

## Installing necessary packages

Before beginning, make sure that all the necessary dependencies are installed using R version 3 as follows:

```{r "code0.r"/code0, eval=FALSE}
```

## Configuring global parameters

We first attempt to isolate most of the variable parameters used in the data processing and analysis.  Of particular note, users should set the location of the GWA data set.  Other variables here specify input and output.  Some analysis threshold parameters are also included.

```{r "globals.R"/globals}
```

## Data pre-processing

First we use the `read.plink` function in the *snpStats* package to read the genotype data from the collection of `.bfam`, `.bim` and `.bed` files.  


```{r "code1.r"/code1-a}
```

The `geno` object contains a `genotype` member of type `snpMatrix` where each column is a SNP and each row is a sample.  For convenience, we assign that to the variable `genotype`.  `geno` also contains a table of information about each sample, called a `map` that we assign to `genoBim`.

```{r "code1.r"/code1-b}
```

Supplemental clinical data is found in a corresponding CSV file for each sample ID.
```{r "code1.r"/code1-c}
```

We then filter the genotype data to only include samples with corresponding clinical data by indexing the SnpMatrix using the sample IDs, which match the row names.

```{r "code1.r"/code1-d}
```

## SNP level filtering

Once the data is loaded, we are ready to remove SNPs that fail to meet
minimum criteria due to missing data, low variability or genotyping
errors.  *snpStats* provides `col.summary` and `row.summary` functions
that return statistics on SNPs and samples, respectively.

```{r "code2.r"/code2-a}
```

Using these summary statistics, we keep the subset of SNPs that our criteria for minimum call rate and variability.

```{r "code2.r"/code2-b}
```

## Sample level filtering

The second stage of data pre-processing involves filtering samples,
i.e. removing individuals due to missing data, sample contamination,
correlation (for population-based investigations) and racial/ethnic or
gender ambiguity or discordance.  In our study, we address these
issues by filtering on call rate, heterozygosity, cryptic relatednes
and duplicates using identity-by-descent, and we visually assess
ancestry.

### Basic sample filtering

Filtering samples is achieved using `row.summary`, 
augmented by an additional heterozygosity F statistic.

```{r "code3.r"/code3-a, cache.lazy=FALSE, cache.vars='snpsum.row'}
```

We then apply filtering on call rate and heterozygosity, selecting only 
those samples that meet our criteria.

```{r "code3.r"/code3-b}
```

### IBD analysis

In addition to these summary statistics, we also want to filter on
relateness criteria.  We use the *SNPRelate* package to perform
identity-by-descent (IBD) analysis.  This package requires that the
data be transformed into a *GDS* format file.  IBD analysis is
performed on only a subset of SNPs that are in linkage equilibrium by
iteratively removing adjacent SNPs that exceed an LD threshold in a
sliding window (function `snpgdsLDpruning`).

```{r "code3.r"/code3-c}
```

The `snpgdsIBDMoM` function computes the IBD coefficients using method
of moments.  The result is a table indicating kinship among pairs of
samples.

```{r "code3.r"/code3-d}
```

Using the IBD pairwise sample relatedness measure, we iteratively
remove samples that are too similar using a greedy strategy in which
the sample with the largest number of related samples is removed.  The
process is repeated until there are no more pairs of samples with
kinship above our cut-off.

```{r "code3.r"/code3-e}
```

### Ancestry

To better understand ancestry, we plot the first two principal
components of the genotype data.

```{r "code3.r"/code3-f}
```

## SNP Filtering - HWE filtering on control samples

Finally, once samples are filtered, we return to SNP level filtering
and apply a check of Hardy-Weinberg equilibrium for just the study controls
and remove those that exceed our cut-off.

```{r "code3.r"/code3-g}
```

## Re-compute PCA

With our final set of filtered SNPs and samples, we recompute the
principal components again to be used in later model fitting.

```{r "code4.r"/code4-a}
```


## Imputation of SNPs

In addition to the genotyped SNPs from our study, it is useful to
extend the analysis to other known SNPs in our region of interest
(chromsome 16).  We retrieve additional SNPs by reading a PED file
from the 1000 Genomes project.  Then we derive imputation "rules" for
the additional SNPs that were not typed in our study using
`snp.imputation` based on the haplotypes from the 1000 Genomes data.
Each rule is a model for the distribution of the genotypes for each
SNP based on the typed SNPs.  Using these rules, we impute the
genotypes of the additional SNPs for the samples in our study.

```{r "code5.r"/code5-a}
```

## Genome-wide association analysis

Now that our data is loaded, filtered, and additional SNP genotypes
imputed, we are ready to perform genome-wide association analysis.
This involves regressing each SNP separately on a given trait,
adjusted for patient level clinical, demographic and environmental
factors.  Due to the large number of SNPs and the generally
uncharacterized relationships to the outcome, a simple single additive
model will be employed.  A Bonferonni corrected significant threshold
of $5 x 10^-8$ is used to control for family-wide error rate.

Due to the large number of models that require fitting, the GWA
analysis can be deployed in parallel across multiple processors or
machines to reduce the running time.  Here we demonstrate two basic
methods for performing parallel processing using the *doParallel*.
These approaches are demonstrated in the function GWAA:

### GWAA function

```{r "GWAA.R"/gwaa}
```

### Phenotype data preparation

First we create a data.frame of phenotype features that is the
concatenation of clinical features and the first ten principal
components.  The HDL feature is normalized using a rank-based inverse
normal transform.

```{r "code6.r"/code6-a}
```

### Parallel model fitting

Using this phenotype data, we perform model fitting on each of the
SNPs genotyped in the study and write the results to a CSV file.

```{r "code6.r"/code6-b}
```

### Model fitting of non-typed SNPs using rules

We also perform association testing on the additional SNPs from the
1000 Genomes project.  Instead of using a single imputed genotype
value, we use the "rules" derived from the haplotype analysis, above.
The resulting SNPs are combined with the chromosome position
information to create a table of SNPs, location and score.

```{r "code7.r"/code7-a}
```

### Mapping associated SNPs to genes

Using a separate data file containing the chromosome and coordinate
locations of each protein coding gene, we can locate coincident genes
and SNPs with significant association p-values.

We use the following function to extract the subset of SNPs that are
near our gene of interest:

```{r "map2gene.R"/map2gene}
```

And then we call the map2gene function for "CETP" and then filter the
imputed genotypes to extract only those SNPs that are near CETP.

```{r "code7.r"/code7-b}
```

## Post-analytic visualization and genomic integration

We now have generated and fit both typed and imputed genotypes.  The
next step is to combine the results, and isolate just those SNPs in
our region of interest.  Following similar steps as above for imputed
steps, the typed SNPs are loaded from a file generated by the `GWAA`
function and then we follow similar steps to attach chromosome and
position to each SNP, order by significance.

```{r "code8.r"/code8-a}
```

### Isolate CETP-specific SNPs

The two tables of typed and imputed genotypes are combined into a
single table.  In addition, we also concatenate just the SNPs near
CETP and display them all here.

```{r "code8.r"/code8-b}
```

## Visualization and QC ##

Several plots allow us both to visualize the GWA analysis findings
while performing quality control checks.  Specifically, we are
interested in identifying data inconsistencies, potential systemic
biases and redundancies of our findings with previously reported
results.

### Manhattan plot ###

Manhattan plots are used to visual GWA significant results by
chromosome location.  We will use this function to plot a set of
SNPs across the genome.

```{r "GWAS_ManhattanFunction.R"/manhattan}
```

```{r "code9.r"/code9-a}
```

### Quantile-quantile plots and the $\lambda$-statistic ### 

Q-Q plots are used to visualize the relationship between the expected
and observed distributions of SNP level test statistics.  Here we
compare these statistics for the unadjusted model (left) compared with
the model adjusted for confounders by incorporating the first ten
principal compoents along with sex, age and CAD.

A new set of models is generated with only the phenotype (HDL) and no
additional factors.  The results are plotted using the `GenABEL`
package's `estlambda` function.

```{r "code9.r"/code9-b}
```

We see here that the tail of the distribution is brough closer to the
y=x line after accounting for confounding by race/ethnicity in the
modeling framework.  If the data in this figure were shifted up or
down from the $y=x$ line, then we would want to investigate some form
of systemic bias.  The degree of deviation from this line is measured
formally by the $\lambda$-statistic, where a value close to 1 suggests
appropriate adjustment for the potential admixture.  A slight
deviation in the upper right tail from the $y=x$ line suggests crudely
that some form of association is present in the data.  There is only a
slight improvement in $\lambda$ between the unadjusted model and the
model with PCs indicating that the population is relatively
homogenous.

### Heatmap ###

Heatmaps are typically used in the context of GWA analysis to
visualize the linkage disequilibrium pattern between significant SNPs
other SNPs in nearby regions.  Here we include our most significant
SNP from our analysis and other SNPs near CETP.  The darker shading
indicates higher LD.  The plot also includes $-log_{10}(p)$ values to
illustrate their connection with physical location.

```{r "code9.r"/code9-c}
```

